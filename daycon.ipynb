{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "daycon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13F4GqFbfk3wIntbqsTk4rK4Ijw-TjmDb",
      "authorship_tag": "ABX9TyNUj2+sBh847yBKdW8XnRVB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/convin305/sum-tudy/blob/master/daycon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QucnFR5BJiV"
      },
      "source": [
        "# 수정사항\r\n",
        "#### 이름 : 수정사항 을 수정 후 기록하기\r\n",
        "##### 만약에 cvs제출하면 맨 밑에 파일이름 뒤에 숫자 하나씩 증가시키겠습니다~!  \r\n",
        "##### (팀장이 제출하라해서...제가 제출할때마다 바꿔서 할게요)\r\n",
        "_____________________________  \r\n",
        "* 수민 : resnet18을 resnet50으로 변경, cuda 메모리 부족으로 아직 돌리지는 않음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UWK0EuWxB6W",
        "outputId": "48563d3d-b038-4664-d941-b2d6cc70b770"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUkuoSejeVRQ"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import cv2\r\n",
        "from tqdm import tqdm\r\n",
        "import imutils\r\n",
        "import zipfile\r\n",
        "import os\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.models as models\r\n",
        "import torchvision.transforms as T\r\n",
        "from torch.utils.data import DataLoader, Dataset\r\n",
        "from google.colab import output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_ZvJG0dJiYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bc2123-0f4e-4dff-876e-5697626a779d"
      },
      "source": [
        "!mkdir \"./dirty_mnist\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./dirty_mnist’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrqJKQYZEYGL",
        "outputId": "ed32d682-eea6-4be4-8159-bd8891f38586"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/Data/dirty_mnist_2nd.zip\" -d \"./dirty_mnist/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Data/dirty_mnist_2nd.zip\n",
            "replace ./dirty_mnist/00000.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELVQTkFwuTn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d78411c-411c-4824-e17a-5a357fcf1ce3"
      },
      "source": [
        "!mkdir \"./test_dirty_mnist\"\r\n",
        "!unzip \"/content/drive/MyDrive/Data/test_dirty_mnist_2nd.zip\" -d \"./test_dirty_mnist/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./test_dirty_mnist’: File exists\n",
            "Archive:  /content/drive/MyDrive/Data/test_dirty_mnist_2nd.zip\n",
            "replace ./test_dirty_mnist/50000.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlOy1w7fuThc"
      },
      "source": [
        "dirty_mnist_answer = pd.read_csv(\"/content/drive/MyDrive/Data/dirty_mnist_2nd_answer.csv\")\r\n",
        "namelist = os.listdir('./dirty_mnist/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi_MVu7FuTeg"
      },
      "source": [
        "class ToTensor(object):\r\n",
        "  def __call__(self, sample):\r\n",
        "    image, label = sample['image'], sample['label']\r\n",
        "    image = image.transpose((2, 0, 1))\r\n",
        "    return {'image': torch.FloatTensor(image),\r\n",
        "            'label': torch.FloatTensor(label)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnSqoe8jKmAs"
      },
      "source": [
        "to_tensor = T.Compose([\r\n",
        "                       ToTensor()\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JrBgPlFKqbx"
      },
      "source": [
        "class DatasetMNIST(torch.utils.data.Dataset):\r\n",
        "    def __init__(self,\r\n",
        "                 dir_path,\r\n",
        "                 meta_df,\r\n",
        "                 transforms=to_tensor,#미리 선언한 to_tensor를 transforms로 받음\r\n",
        "                 augmentations=None):\r\n",
        "        \r\n",
        "        self.dir_path = dir_path # 데이터의 이미지가 저장된 디렉터리 경로\r\n",
        "        self.meta_df = meta_df # 데이터의 인덱스와 정답지가 들어있는 DataFrame\r\n",
        "\r\n",
        "        self.transforms = transforms# Transform\r\n",
        "        self.augmentations = augmentations # Augmentation\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.meta_df)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        # 폴더 경로 + 이미지 이름 + .png => 파일의 경로\r\n",
        "        # 참고) \"12\".zfill(5) => 000012\r\n",
        "        #       \"146\".zfill(5) => 000145\r\n",
        "        # cv2.IMREAD_GRAYSCALE : png파일을 채널이 1개인 GRAYSCALE로 읽음\r\n",
        "        image = cv2.imread(self.dir_path +\\\r\n",
        "                           str(self.meta_df.iloc[index,0]).zfill(5) + '.png',\r\n",
        "                           cv2.IMREAD_GRAYSCALE)\r\n",
        "        # 0 ~ 255의 값을 갖고 크기가 (256,256)인 numpy array를\r\n",
        "        # 0 ~ 1 사이의 실수를 갖고 크기가 (256,256,1)인 numpy array로 변환\r\n",
        "        image = (image/255).astype('float')[..., np.newaxis]\r\n",
        "\r\n",
        "        # 정답 numpy array생성(존재하면 1 없으면 0)\r\n",
        "        label = self.meta_df.iloc[index, 1:].values.astype('float')\r\n",
        "        sample = {'image': image, 'label': label}\r\n",
        "\r\n",
        "        # transform 적용\r\n",
        "        # numpy to tensor\r\n",
        "        if self.transforms:\r\n",
        "            sample = self.transforms(sample)\r\n",
        "\r\n",
        "        # sample 반환\r\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjaKqlfaLRZd"
      },
      "source": [
        "모델 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWwS3QLpLREr"
      },
      "source": [
        "class MultiLabelResnet(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(MultiLabelResnet, self).__init__()\r\n",
        "        self.conv2d = nn.Conv2d(1, 3, 3, stride=1)\r\n",
        "        self.resnet = models.resnet50() \r\n",
        "        self.FC = nn.Linear(1000, 26)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "      x = F.relu(self.conv2d(x))\r\n",
        "      x = F.relu(self.resnet(x))\r\n",
        "      x = torch.sigmoid(self.FC(x))\r\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VySst7MLf58",
        "outputId": "d8a97c8b-d012-4bab-b0d7-5616e940e41d"
      },
      "source": [
        "model = MultiLabelResnet()\r\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelResnet(\n",
              "  (conv2d): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (resnet): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              "  )\n",
              "  (FC): Linear(in_features=1000, out_features=26, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsIvAUAaLjME"
      },
      "source": [
        "학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksCJeA7LLjzk"
      },
      "source": [
        "from sklearn.model_selection import KFold\r\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDfYZE_nN6UW"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "H0uT8zGOLPIy",
        "outputId": "cad3a902-e700-4748-a75b-207eb1a926e6"
      },
      "source": [
        "best_models = []\r\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(dirty_mnist_answer),1):\r\n",
        "  print(f'[fold: {fold_index}]')\r\n",
        "  torch.cuda.empty_cache()\r\n",
        "  train_answer = dirty_mnist_answer.iloc[trn_idx]\r\n",
        "  test_answer  = dirty_mnist_answer.iloc[val_idx]\r\n",
        "  #Data set\r\n",
        "  train_dataset = DatasetMNIST(\"dirty_mnist/\", train_answer)\r\n",
        "  valid_dataset = DatasetMNIST(\"dirty_mnist/\", test_answer)\r\n",
        "  #Data Loader\r\n",
        "  train_data_loader = torch.utils.data.DataLoader(train_dataset,batch_size = 128,shuffle = False,num_workers = 3)\r\n",
        "  valid_data_loader = torch.utils.data.DataLoader(valid_dataset,batch_size = 32,shuffle = False,num_workers = 3)\r\n",
        "\r\n",
        "  model = MultiLabelResnet()\r\n",
        "  model.to(device)\r\n",
        "\r\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\r\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size = 5,gamma = 0.75)\r\n",
        "  criterion = torch.nn.BCELoss()\r\n",
        "\r\n",
        "  valid_acc_max = 0\r\n",
        "  for epoch in range(10):\r\n",
        "    train_acc_list = []\r\n",
        "    with tqdm(train_data_loader,total=train_data_loader.__len__()) as train_bar:\r\n",
        "      for sample in train_bar:\r\n",
        "        train_bar.set_description(f\"Train Epoch {epoch}\")\r\n",
        "        optimizer.zero_grad()\r\n",
        "        images, labels = sample['image'], sample['label']\r\n",
        "        images = images.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        model.train()\r\n",
        "        with torch.set_grad_enabled(True):\r\n",
        "          probs  = model(images)\r\n",
        "          loss = criterion(probs, labels)\r\n",
        "          loss.backward()\r\n",
        "          optimizer.step()\r\n",
        "\r\n",
        "          probs  = probs.cpu().detach().numpy()\r\n",
        "          labels = labels.cpu().detach().numpy()\r\n",
        "          preds = probs > 0.5\r\n",
        "          batch_acc = (labels == preds).mean()\r\n",
        "          train_acc_list.append(batch_acc)\r\n",
        "          train_acc = np.mean(train_acc_list)\r\n",
        "\r\n",
        "        train_bar.set_postfix(train_loss= loss.item(),train_acc = train_acc)\r\n",
        "    valid_acc_list = []\r\n",
        "    with tqdm(valid_data_loader,total=valid_data_loader.__len__(),unit=\"batch\") as valid_bar:\r\n",
        "      for sample in valid_bar:\r\n",
        "        valid_bar.set_description(f\"Valid Epoch {epoch}\")\r\n",
        "        optimizer.zero_grad()\r\n",
        "        images, labels = sample['image'], sample['label']\r\n",
        "        images = images.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        model.eval()\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "          probs  = model(images)\r\n",
        "          valid_loss = criterion(probs, labels)\r\n",
        "\r\n",
        "          probs  = probs.cpu().detach().numpy()\r\n",
        "          labels = labels.cpu().detach().numpy()\r\n",
        "          preds = probs > 0.5\r\n",
        "          batch_acc = (labels == preds).mean()\r\n",
        "          valid_acc_list.append(batch_acc)\r\n",
        "\r\n",
        "        valid_acc = np.mean(valid_acc_list)\r\n",
        "        valid_bar.set_postfix(valid_loss = valid_loss.item(),valid_acc = valid_acc)\r\n",
        "\r\n",
        "    lr_scheduler.step()\r\n",
        "    if valid_acc_max < valid_acc:\r\n",
        "       valid_acc_max = valid_acc\r\n",
        "       best_model = model\r\n",
        "       MODEL = \"resnet50\"\r\n",
        "       path = \"/content/drive/MyDrive/Daycon\"\r\n",
        "       torch.save(best_model, f'{path}{fold_index}_{MODEL}_{valid_loss.item():2.4f}_epoch_{epoch}.pth')\r\n",
        "\r\n",
        "best_models.append(best_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3c53e60b3d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfold_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirty_mnist_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[fold: {fold_index}]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtrain_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirty_mnist_answer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'kfold' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiUsmsTNcLlS"
      },
      "source": [
        "sample_images = images.cpu().detach().numpy()\r\n",
        "sample_prob = probs\r\n",
        "sample_labels = labels\r\n",
        "\r\n",
        "idx = 1\r\n",
        "plt.imshow(sample_images[idx][0])\r\n",
        "plt.title(\"sample input image\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print('예측값 : ',dirty_mnist_answer.columns[1:][sample_prob[idx] > 0.5])\r\n",
        "print('정답값 : ', dirty_mnist_answer.columns[1:][sample_labels[idx] > 0.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaWvID1fcQJd"
      },
      "source": [
        "#test Dataset 정의\r\n",
        "sample_submission = pd.read_csv(\"/content/drive/MyDrive/Data/sample_submission.csv\")\r\n",
        "test_dataset = DatasetMNIST(\"test_dirty_mnist/\", sample_submission)\r\n",
        "batch_size = 128\r\n",
        "test_data_loader = DataLoader(\r\n",
        "    test_dataset,\r\n",
        "    batch_size = batch_size,\r\n",
        "    shuffle = False,\r\n",
        "    num_workers = 3,\r\n",
        "    drop_last = False\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnU93rducRlx"
      },
      "source": [
        "predictions_list = []\r\n",
        "# 배치 단위로 추론\r\n",
        "prediction_df = pd.read_csv(\"/content/drive/MyDrive/Data/sample_submission.csv\")\r\n",
        "\r\n",
        "# 5개의 fold마다 가장 좋은 모델을 이용하여 예측\r\n",
        "for model in best_models:\r\n",
        "    # 0으로 채워진 array 생성\r\n",
        "    prediction_array = np.zeros([prediction_df.shape[0],\r\n",
        "                                 prediction_df.shape[1] -1])\r\n",
        "    for idx, sample in enumerate(test_data_loader):\r\n",
        "        with torch.no_grad():\r\n",
        "            # 추론\r\n",
        "            model.eval()\r\n",
        "            images = sample['image']\r\n",
        "            images = images.to(device)\r\n",
        "            probs  = model(images)\r\n",
        "            probs = probs.cpu().detach().numpy()\r\n",
        "            preds = (probs > 0.5)\r\n",
        "\r\n",
        "            # 예측 결과를 \r\n",
        "            # prediction_array에 입력\r\n",
        "            batch_index = batch_size * idx\r\n",
        "            prediction_array[batch_index: batch_index + images.shape[0],:]\\\r\n",
        "                         = preds.astype(int)\r\n",
        "                         \r\n",
        "    # 채널을 하나 추가하여 list에 append\r\n",
        "    predictions_list.append(prediction_array[...,np.newaxis])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voArgGc1cVIq"
      },
      "source": [
        "# axis = 2를 기준으로 평균\r\n",
        "predictions_array = np.concatenate(predictions_list, axis = 2)\r\n",
        "predictions_mean = predictions_array.mean(axis = 2)\r\n",
        "\r\n",
        "# 평균 값이 0.5보다 클 경우 1 작으면 0\r\n",
        "predictions_mean = (predictions_mean > 0.5) * 1\r\n",
        "predictions_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGI5krFscYEP"
      },
      "source": [
        "sample_submission = pd.read_csv(\"/content/drive/MyDrive/Data/sample_submission.csv\")\r\n",
        "sample_submission.iloc[:,1:] = predictions_mean\r\n",
        "sample_submission.to_csv(\"baseline_prediction.csv\", index = False)\r\n",
        "sample_submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0pvAJNw6weq"
      },
      "source": [
        "from google.colab import files\r\n",
        "\r\n",
        "files.download(\"som_multilable_prediction_1\".csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}